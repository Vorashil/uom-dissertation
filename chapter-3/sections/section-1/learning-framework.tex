\section{Learning Framework}

In statistical learning theory, a \emph{learning problem (LP)} provides a formal framework for inferring predictive rules from data~\cite{StatisticalLearningTheory}. This dissertation focuses on the setting of binary classification, where such a rule is a \emph{hypothesis}—a function that assigns a binary label to each data point, or \emph{instance}. The learning framework is defined by four core components: an instance space containing all possible instances, a hypothesis space of candidate functions, a sample space of labeled instances, and a set of probability distributions that model the data-generating process. The ultimate objective is to design a \emph{learning function} that, given a sample of data, selects a hypothesis that generalises well to new, unseen examples.


Formally, a learning problem is given by a tuple
\[
    (\mathcal{X}, \Sigma_{\mathcal{Z}}, \mathcal{D}, \mathcal{H}),
\]
where the components are defined as follows.

\begin{itemize}
    \item $\mathcal{X}$ is a non-empty set, called the \emph{instance space}. Elements of $\mathcal{X}$ represent the objects or inputs under consideration.

    \item $\Sigma_{\mathcal{Z}}$ is a $\sigma$-algebra on the \emph{sample space} $\mathcal{Z}$. The sample space is defined as
    \[
        \mathcal{Z} = \mathcal{X} \times \{0,1\},
    \]
    where $\{0,1\}$ denotes the set of labels.
    We require that $\mathcal{P}_{\mathrm{fin}}(\mathcal{Z}) \subseteq \Sigma_{\mathcal{Z}}$, meaning that every finite subset of $\mathcal{Z}$ is measurable.

    \item $\mathcal{D}$ is a subset of the set $\mathcal{D}^*$, which contains all \emph{probability distributions} defined on the measurable space $(\mathcal{Z}, \Sigma_{\mathcal{Z}})$.

    \item $\mathcal{H}$ is a non-empty set, called the \emph{hypothesis space}, consisting of candidate functions that map inputs to labels:
    \[
        \mathcal{H} \subseteq \{0,1\}^\mathcal{X} = \{h : \mathcal{X} \to \{0,1\}\}.
    \]
    An element $h \in \mathcal{H}$ is called a \emph{hypothesis}.
    The \emph{graph} of $h$ is defined by
    \[
        \Gamma(h) = \{(x,y) \in \mathcal{Z} \mid h(x) = y\} \subseteq \mathcal{Z}.
    \]
\end{itemize}

\medskip

Learning in this framework is based on processing finitely many samples
\[
    (x_1,y_1),\dots,(x_m,y_m) \in \mathcal{Z},
\]
which are assumed to be drawn independently at random according to some distribution $\mathbb{D} \in \mathcal{D}$.
Equivalently, the whole sample may be viewed as an element of $\mathcal{Z}^m$, distributed according to the product measure $\mathbb{D}^m$ on the measurable space $(\mathcal{Z}^m,\Sigma_{\mathcal{Z}}^m)$.

To ensure that such finite samples are measurable, it is natural to require that singletons $\{z\}$ are measurable for every $z \in \mathcal{Z}$. Indeed, measurability of singletons implies that every finite set of sample points is measurable, since finite unions of measurable sets remain measurable. This condition is automatically satisfied, for example, when $\mathcal{Z}$ is a Hausdorff space equipped with its Borel $\sigma$-algebra.

\begin{remark}
    \label{rem:hyposthesis-is-measurable}
    In this chapter, we assume that for every hypothesis $h \in \mathcal{H}$, its graph is measurable, i.e.
    \[
        \Gamma(h) \in \Sigma_{\mathcal{Z}}.
    \]
    This assumption will help us to measure the probability of a hypothesis $h \in \mathcal{H}$ to misclassify instances in~\ref{subsec:evaluating-hypothesis}.
\end{remark}

\begin{remarknl}[Agnostic vs. deterministic learning models]
    In our framework, probability distributions $\mathbb{D}$ are defined on the sample space $\mathcal{Z} = \mathcal{X}\times\{0,1\}$.
    This means that, for a fixed instance $x \in \mathcal{X}$, we have $(x,0)$ and $(x,1)$ as separate sample points, and it's possible that they both have non-zero probability simultanously according to some distribution defined on $(\mathcal{Z}, \Sigma_{\mathcal{Z}})$.
    In other words, the label of $x$ is itself random and not determined by a fixed target function.
    This setting is known as the \emph{agnostic}~\cite[p.45, \S 3.2.1]{UnderstandinMachineLearning}.
    It allows us to capture noisy labels and corrupted data, reflecting the fact that real-world classification tasks may not admit a perfect underlying rule.

    By contrast, in the \emph{deterministic model}~\cite{LearnabilityDeterministic}, the distribution $\mathbb{P}$ is only placed on the instance space $\mathcal{X}$ itself.
    The labels are then determined by a fixed but unknown target function $t \in \{0,1\}^\mathcal{X}$.
    Thus the data are of the form $(x_i,t(x_i))$, where the $x_i$ are drawn according to some probability distribution defined on $\sigma$-algebra on $\mathcal{X}$.

    The distinction between these two paradigms lies in whether label uncertainty is treated as intrinsic (agnostic case) or entirely due to the randomness of instance selection (deterministic case).
\end{remarknl}

\subsection{Evaluating a Learned Hypothesis}\label{subsec:evaluating-hypothesis}

We now make the discussion of learning more rigorous and to introduce the central notions needed for the formal study of Probably Approximately Correct (PAC) learning. PAC learning is not itself a learning function—it is a framework that specifies conditions under which any learning function succeeds. To arrive at this definition, we first need a precise way of evaluating the quality of hypotheses chosen by a learning function. In particular, we distinguish between the \emph{true error}, which measures performance with respect to the underlying distribution, and the \emph{sample error}, which measures performance on a finite dataset. These notions allow us to formalise the idea that a hypothesis may look good on the training data yet perform poorly on unseen examples. We also define the notion of \emph{optimal error}, which represents the best performance achievable within a given hypothesis space. Together, these definitions provide the language required to analyse concrete learning strategies. As an illustration, we will consider the sample error minimisation (SEM~\cite{KrappWirth2021}) framework (also known as empirical risk minimisation (ERM) in~\cite{UnderstandinMachineLearning}), where the learning function selects the hypothesis that minimises the sample error. While intuitive and computationally straightforward, this strategy highlights the gap between sample and true performance, motivating the need for stronger guarantees such as those provided by the PAC framework, which will be developed in the following sections.


%\begin{example}
%    \label{ex:learning-function}
%    Let $\mathcal{X}=\{a,b,c\}$ be a finite instance space, so the sample space is $\mathcal{Z}=\mathcal{X}\times\{0,1\}$.
%    The hypothesis space $\mathcal{H}$ consists of all functions $h:\mathcal{X}\to\{0,1\}$, of which there are $2^{|\mathcal{X}|}=8$ in total.
%    For illustration, we display a few of them in the table below:
%
%    \[
%        \begin{array}{c|ccc}
%            h & a & b & c \\
%            \hline
%            h_0 & 0 & 0 & 0 \\
%            h_1 & 1 & 1 & 1 \\
%            h_{ab} & 0 & 0 & 1 \\
%            h_{bc} & 1 & 0 & 0 \\
%        \end{array}
%    \]
%
%    Suppose we observe a finite sample
%    \[
%        \overline{z} = \bigl((a,0),(b,0),(c,1)\bigr) \in \mathcal{Z}^3,
%    \]
%    which is drawn according to some distribution $\mathbb{D}\in\mathcal{D}$.
%    A learning function $\mathcal{A}$ then selects a hypothesis from $\mathcal{H}$ that is consistent with this sample.
%    In this case, $\mathcal{A}(\overline{z})=h_{ab}$ would be a natural choice, since $h_{ab}$ agrees with the observed labels on $a$, $b$, and $c$.
%
%\end{example}
%
%This example~\ref{ex:learning-function} illustrates the general mechanism of learning: a learning function $\mathcal{A}$ takes a finite sample and outputs a hypothesis $h\in\mathcal{H}$.


%\subsection{Error of a Hypothesis}\label{subsec:error-of-hypothesis}

\subsubsection{Performance of a hypothesis}

The performance of a hypothesis is evaluated by how well it predicts labels of unseen data drawn from the distribution.
This is formalised through the notion of \emph{error}.
We distinguish between the \emph{true error}, which depends on the underlying distribution, and the \emph{sample error}, which is based only on the observed data. Recall that, by our assumption in the remark~\ref{rem:hyposthesis-is-measurable}, the graph of a hypothesis is measurable. This is required for the definition of true error to be well-defined.

\begin{definition}[True error]
    Let $\mathbb{D} \in \mathcal{D}$ be a distribution on $(\mathcal{Z},\Sigma_{\mathcal{Z}})$
    and let $h \in \mathcal{H}$ be a hypothesis.
    The \emph{true error} of $h$ with respect to $\mathbb{D}$ is defined as
    \[
        \operatorname{er}_{\mathbb{D}}(h) \coloneqq \mathbb{D}(\{(x,y)\in\mathcal{Z} \mid h(x)\neq y\}) = \mathbb{D}(\mathcal{Z}\setminus\Gamma(h)).
    \]
\end{definition}

%\noindent
%Thus, $\operatorname{er}_{\mathbb{D}}(h)$ is the probability of misclassification under the distribution $\mathbb{D}$.
%It quantifies the long-run frequency with which $h$ makes mistakes if we were to draw infinitely many samples.

\medskip

The following definition, invokes the other assumption about singletons $\{z\}\in \mathcal{Z}$ being measurable.

\begin{definition}[Sample error]
    Let $z = (z_1,\dots,z_m)\in\mathcal{Z}^m$ be a finite sample and $h\in\mathcal{H}$.
    The \emph{sample error} (also called \emph{empirical error}) of $h$ on $z$ is defined as
    \[
        \hat{\operatorname{er}}_{z}(h) := \frac{1}{m}\sum_{i=1}^m \ell(h,z_i),
    \]
    where the \emph{loss function} $\ell:\mathcal{H}\times \mathcal{Z}\to\{0,1\}$ is given by
    \[
        \ell(h,(x,y)) =
        \begin{cases}
            1 & \text{if } h(x)\neq y, \\
            0 & \text{otherwise}.
        \end{cases}
    \]
\end{definition}

The following lemma, shows that the sample error of a hypothesis $h \in \mathcal{H}$ is measurable function, assuming $\Gamma(h) \in \Sigma_{\mathcal{Z}}$.

\begin{lemma}[Measurability of the Sample Error Map]
    \label{lem:sample-error-measurable}
    Let $h \in \mathcal{H}$ be a hypothesis with a measurable graph $\Gamma(h)$. The sample error map,
    \[
        z \mapsto \hat{\operatorname{er}}_{z}(h),
    \]
    is a $\Sigma_{\mathcal{Z}}^m$-measurable function from $\mathcal{Z}^m$ to $[0,1]$.
\end{lemma}

\begin{proof}

    The sample error map $z \mapsto \frac{1}{m}\sum_{i=1}^m \ell(h,z_i)$ is a linear combination of the functions $f_i(z) = \ell(h,z_i)$. Each function $f_i$ is the composition $\ell(h, \cdot) \circ \pi_i$ of the loss function with the $i$-th coordinate projection map $\pi_i: \mathcal{Z}^m \to \mathcal{Z}$.

    The loss function $\ell(h, \cdot)$ is measurable because it is the indicator of the measurable set $\mathcal{Z}\setminus\Gamma(h)$. The projection map $\pi_i$ is measurable by the definition of the product $\sigma$-algebra $\Sigma_{\mathcal{Z}}^m$. The composition of measurable functions is measurable, and so is a finite sum of such functions~\ref{subsec:measurable-functions}. Therefore, the sample error map is measurable~\cite[Prop 2.4]{FollandRealAnalysis}.
    \qedhere
\end{proof}

The sample error $\hat{\operatorname{er}}_z(h)$ measures how often $h$ disagrees with the observed labels in the dataset $\overline{z} \in \mathcal{Z}$. Unlike the true error, it is always computable from the available data. The natural question is how the sample error relates to the true error. The next lemma establishes a basic relationship: the sample error is an unbiased estimator of the true error.

\begin{lemma}[Sample error is an unbiased estimator]
    \label{lem:sample-error-unbiased}
    Let $h \in \mathcal{H}$ be a hypothesis with a measurable graph $\Gamma(h)$. For a sample $z = (z_1, \dots, z_m)$ drawn according to the product distribution $\mathbb{D}^m$, the expected value of the sample error equals the true error:
    \[
        \mathbb{E}_{z\sim\mathbb{D}^m}\bigl[\hat{\operatorname{er}}_{z}(h)\bigr] = \operatorname{er}_{\mathbb{D}}(h).
    \]
\end{lemma}

\begin{proof}

    By linearity of expectation and the definition of sample error, we have
    \[
        \mathbb{E}_{z\sim\mathbb{D}^m}\bigl[\hat{\operatorname{er}}_{z}(h)\bigr] = \frac{1}{m}\sum_{i=1}^m \mathbb{E}_{z_i\sim\mathbb{D}}\bigl[\ell(h,z_i)\bigr].
    \]
    For any $i \in \{1, \dots, m\}$, this expectation is taken over the single-variable distribution $\mathbb{D}$:
    \[
        \mathbb{E}_{z_i\sim\mathbb{D}}\bigl[\ell(h,\cdot)\bigr] = \mathbb{E}_{z_i\sim\mathbb{D}}\bigl[\mathds{1}_{\mathcal{Z}\setminus\Gamma(h)}\bigr] = \mathbb{D}(\mathcal{Z}\setminus\Gamma(h))=\operatorname{er}_{\mathbb{D}}(h).
    \]
    Substituting this into the sum completes the proof:
    \[
        \mathbb{E}_{z\sim\mathbb{D}^m}\bigl[\hat{\operatorname{er}}_{z}(h)\bigr] = \frac{1}{m}\sum_{i=1}^m \operatorname{er}_{\mathbb{D}}(h) = \operatorname{er}_{\mathbb{D}}(h).
    \]
    \qedhere
\end{proof}

\begin{remarknl}[Why unbiasedness is not enough for learning]
    \label{rem:unbiased-not-enough}
    
    While Lemma~\ref{lem:sample-error-unbiased} shows that the sample error is an unbiased estimator—meaning it is correct "on average"—this property alone is insufficient for learning. To understand why, consider the following critical points:
    
    First, the unbiasedness property holds for each hypothesis $h$ individually. When we fix a specific $h$ and draw many independent samples, the average of their sample errors converges to the true error by the weak law of large numbers~\cite[Chap.~9]{MeasureTheoryLeGall}. However, in learning, we do not fix $h$ in advance. Instead, a learning algorithm selects $h$ based on the observed data, typically by choosing the hypothesis with the smallest sample error.
    
    This data-dependent selection creates a fundamental problem: the hypothesis that looks best on the sample may have been "lucky" with that particular dataset. For instance, consider a hypothesis $h^*$ that happens to correctly classify all points in our sample purely by chance, yielding $\hat{\operatorname{er}}_z(h^*) = 0$. While the expected value $\mathbb{E}[\hat{\operatorname{er}}_z(h^*)]$ equals the true error $\operatorname{er}_{\mathbb{D}}(h^*)$, for this specific sample $z$, the gap $|\hat{\operatorname{er}}_z(h^*) - \operatorname{er}_{\mathbb{D}}(h^*)|$ could be large.
    
    The problem becomes more severe when we have many hypotheses. With a large hypothesis space $\mathcal{H}$, it becomes increasingly likely that at least one hypothesis will appear misleadingly good on any given sample. This is why we need a stronger property that controls the approximation error simultaneously for all hypotheses—this is precisely what uniform convergence provides, as we will see in Section~\ref{sec:pac-learning}.
\end{remarknl}

\begin{definition}[Optimal error]
    The \emph{optimal true error} achievable by the hypothesis space $\mathcal{H}$ under $\mathbb{D}$ is
    \[
        \operatorname{opt}_{\mathbb{D}}(\mathcal{H}) := \inf_{h\in\mathcal{H}} \operatorname{er}_{\mathbb{D}}(h).
    \]
    Similarly, given a finite sample $z \in \mathcal{Z}^m$, the \emph{optimal sample error} is
    \[
        \operatorname{opt}_{z}(\mathcal{H}) := \min_{h\in\mathcal{H}} \hat{\operatorname{er}}_{z}(h).
    \]
\end{definition}

\medskip

The true error is the performance of a hypothesis on the underlying distribution, but it is typically inaccessible.
The sample error, by contrast, is observable but may deviate from the true error due to randomness in sampling.
The optimal errors describe the best performance achievable by any hypothesis in $\mathcal{H}$, either with respect to $\mathbb{D}$ (idealised) or with respect to the observed dataset (computable).

\medskip

Formally, the learning function $\mathcal{A}: \cup_{n \in \mathbb{N}} \mathcal{Z}^n \to \mathcal{H}$ takes the finite sample $\overline{z} \in \mathcal{Z}^m$ and returns a candidate hypothesis $\mathcal{A}(\overline{z}) \in \mathcal{H}$ that should explain the observed labels and hopefully predict new ones correctly. What distinguishes the PAC viewpoint is a quantitative requirement: for given $\varepsilon,\delta\in(0,1)$, if the sample size $m$ is large enough, then with probability at least $1-\delta$ (over the random draw of the sample) the hypothesis $\mathcal{A}(\overline{z})$ misclassifies new points with probability at most $\varepsilon$. Here$\varepsilon$ is the \emph{accuracy parameter} (“approximately”),
while $\delta$ is the \emph{confidence parameter} (“probably”).~\cite[Sec 3.1, p.43]{UnderstandinMachineLearning}
A fully formal definition of PAC learning will be given in~\ref{sec:pac-learning} and will be central to the rest of the dissertation.





