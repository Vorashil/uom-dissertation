\section{Proof of the Fundamental Theorem of Statistical Learning}

In this section, we provide the proof for the "only if" direction of the Fundamental Theorem of Statistical Learning (\ref{thm:fundamental-theorem}). We will then briefly outline the argument for the converse, which relies on the Uniform Convergence Property.

\subsection{PAC Learnability Implies Finite VC Dimension}

We will prove the following statement, which is one half of the Fundamental Theorem.

\begin{theorem}
    Let the learning framework $(\mathcal{X}, \Sigma_{\mathcal{Z}}, \mathcal{D}, \mathcal{H})$ be given as in Theorem~\ref{thm:fundamental-theorem}. If $\mathcal{H}$ is PAC learnable, then $\operatorname{vc}(\mathcal{H}) < \infty$.
\end{theorem}

\begin{proof}

    The proof proceeds by contrapositive. We assume that $\mathcal{H}$ has an infinite VC dimension and show that it cannot be PAC learnable. To show that $\mathcal{H}$ is not PAC learnable, we must negate its definition. That is, we must show that there exist fixed accuracy and confidence parameters, $\varepsilon > 0$ and $\delta > 0$, such that for any sample size $m \in \mathbb{N}$, there exists a distribution $\mathbb{D} \in \mathcal{D}$ for which any learning algorithm $\mathcal{A}$ fails to meet the PAC guarantee. Specifically, the probability of the algorithm returning a hypothesis with an excess error greater than $\varepsilon$ will be at least $\delta$.

    Let us fix the PAC parameters $\varepsilon = 1/8$ and $\delta = 1/7$. Let $\mathcal{A}$ be an arbitrary learning function and $m \in \mathbb{N}$ be any sample size. Our goal is to find a distribution $\mathbb{D} \in \mathcal{D}$ such that the hypothesis $\mathcal{A}(\overline{z})$ returned by the algorithm on a sample $\overline{z} \sim \mathbb{D}^m$ fails the PAC condition, i.e.,
    \[
        \mathbb{D}^m\left(\left\{ \overline{z} \in \mathcal{Z}^m \mid \operatorname{er}_{\mathbb{D}}(\mathcal{A}(\overline{z})) - \operatorname{opt}_{\mathbb{D}}(\mathcal{H}) > \frac{1}{8} \right\}\right) \ge \frac{1}{7}.
    \]
    This will demonstrate that no sample size $m_0$ can satisfy the PAC definition for these fixed $\varepsilon$ and $\delta$, proving that $\mathcal{H}$ is not PAC learnable.

    First, we use the assumption of an infinite VC dimension to construct a specific learning problem that is designed to be difficult for any algorithm. Since $\operatorname{vc}(\mathcal{H}) = \infty$, there exists a set of any finite size that can be shattered by $\mathcal{H}$. We choose a set $S = \{x_1, \dots, x_{2m}\} \subseteq \mathcal{X}$ of size $2m$ that is shattered by $\mathcal{H}$.

    Let $\mathcal{F} = \{0,1\}^S$ be the set of all $T \coloneq 2^{2m}$ possible binary functions on $S$. We index each function in $\mathcal{F}$ as
    \[
        \{0,1\}^S = \{f_1, \dots, f_T\}.
    \]
    By the definition of shattering, for each function $f_i \in \mathcal{F}$, there exists a hypothesis $h_i \in \mathcal{H}$ such that its restriction to $S$ is exactly $f_i$, i.e., $h_i|_S = f_i$.

    Next, we use this shattered set to define a family of probability distributions. Let $\mathcal{Z}_S = S \times \{0,1\}$ be the finite sample space restricted to the shattered set $S$. For each $1 \leq i \leq T$, we define a distribution $\mathbb{D}_i$ on the measurable space $(\mathcal{Z}_S, \mathcal{P}(\mathcal{Z}_S))$. This distribution's probability mass function is given by:
    \[
        \mathbb{D}_i(\{z\}) =
        \begin{cases}
            \frac{1}{2m} & \text{if } z \in \Gamma(f_i), \\
            0 & \text{otherwise},
        \end{cases}
    \]
    where $\Gamma(f_i) = \{(x, f_i(x)) \mid x \in S\}$ is the graph of the function $f$ over the set $S$. In other words, $\mathbb{D}_i$ is the discrete uniform distribution on the $2m$ points that constitute the graph of $f_i$. Note that, by definition, $\operatorname{er}_{\mathbb{D}_i}(f_i) = 0$ for all $i \in \{1, \dots, T\}$.

    The PAC condition for our chosen $\varepsilon=1/8$ and a distribution $\mathbb{D}_i$ simplifies to requiring that $\operatorname{er}_{\mathbb{D}_i}(\mathcal{A}(\overline{z})) \le 1/8$. Our strategy is to show that no single algorithm $\mathcal{A}$ can perform well simultaneously for all these distributions. We will show that there is at least one function $f \in \mathcal{F}$ for which the algorithm's expected error is high. The core of the argument is that a sample of size $m$ is insufficient to distinguish between the $2^{2m}$ possible underlying functions, as it is likely to reveal the labels of at most $m$ of the $2m$ points in $S$.

    The following lemma formalises this idea. It states that there must be at least one distribution $\mathbb{D}_j$ for which the learning algorithm $\mathcal{A}$ has a high expected error, where $j \in \{1, \dots, T\}$.

    \begin{lemma}
        \label{lem:no-free-lunch}
        Let $\mathcal{A}_S(\overline{z}) = \mathcal{A}(\overline{z})|_S$ be the restriction of the learned hypothesis to the set $S$. Then the following inequality holds:
        \[
            \max_{1 \leq i \leq T} \Bigl( \mathbb{E}_{\overline{z} \sim \mathbb{D}_i^m} \left[ \operatorname{er}_{\mathbb{D}_i}(\mathcal{A}_S(\overline{z})) \right]\Bigr) \ge \frac{1}{4}.
        \]
    \end{lemma}

    \begin{subproof}[Proof of Lemma~\ref{lem:no-free-lunch}]

        Note that, $\operatorname{er}_{\mathbb{D}_i}(\mathcal{A}(\square)): \mathcal{Z}_S^m\to [0,1]$ is positive and the bounded random variable. Hence, by the definition of well-definedness of expected value~\cite[Def 8.3]{MeasureTheoryLeGall},
        \[
            \mathbb{E}_{\overline{z} \sim \mathbb{D}_i^m}(\operatorname{er}_{\mathbb{D}_i}(\mathcal{A}(\square)))
        \]
        is well-defined.

        Moreover, the maximum of a set of values is always greater than or equal to their average. Therefore, we can find lower-bound for the maximum expected error by the average expected error taken over all $1 \leq i \leq T$:
        \[
            \max_{1 \leq i \leq T} \Bigl( \mathbb{E}_{\overline{z} \sim \mathbb{D}_i^m} \left[ \operatorname{er}_{\mathbb{D}_i}(\mathcal{A}_S(\overline{z})) \right]\Bigr) \ge \frac{1}{T} \sum_{i = 1}^{T} \mathbb{E}_{\overline{z} \sim \mathbb{D}_i^m} \left[ \operatorname{er}_{\mathbb{D}_i}(\mathcal{A}_S(\overline{z})) \right].
        \]
        We first analyze the average on the right-hand side. A sample $\overline{z}$ drawn from $\mathbb{D}_i^m$ consists of $m$ pairs $(x,y)$ where each $x$ is drawn uniformly from $S$ and $y=f_i(x)$. Let $S^m$ denote the set of all possible sequences of $m$ instances drawn from $S$ and $|S^m| = (2m)^m$. Note that, we draw these $m$ instances from $S$ with replacements, hence some $m$-tuples in $S^m$ contain duplicate instances. This observation will be used later in the proof.

        For any sequence of instances $\overline{x} = (x_{j_1}, \dots, x_{j_m}) \in S^m$, denote the corresponding labeled sample under $f_i\in \mathcal{F}$ by
        \[
            \overline{z}_{\overline{x}}^{i} \coloneq ((x_{j_1}, f_i(x_{j_1})), \dots, (x_{j_m}, f_i(x_{j_m}))).
        \]
        The probability of drawing this specific sample is $\mathbb{D}_i^m(\{\overline{z}_{\overline{x}}^{i}\}) = (1/2m)^m$. Therefore,
        \[
            \mathbb{E}_{\overline{z} \sim \mathbb{D}_i^m} \left[ \operatorname{er}_{\mathbb{D}_i}(\mathcal{A}_S(\overline{z})) \right] = \sum_{\overline{x} \in S^m} \frac{1}{(2m)^m} \operatorname{er}_{\mathbb{D}_i}(\mathcal{A}_S(\overline{z}_{\overline{x}}^{i})))
        \]
        for all $i \in \{1, \dots, T\}$.

        We can now rewrite the expectation as a sum over all possible instance sequences $\overline{x} \in S^m$ and then swap the order of summation:
        \begin{align*}
            \frac{1}{T} \sum_{i=1}^{T} \mathbb{E}_{\overline{z} \sim \mathbb{D}_i^m} \left[ \operatorname{er}_{\mathbb{D}_i}(\mathcal{A}_S(\overline{z})) \right] &= \frac{1}{T}  \sum_{i=1}^{T} \left( \sum_{\overline{x} \in S^m} \frac{1}{(2m)^m} \operatorname{er}_{\mathbb{D}_i}(\mathcal{A}_S(\overline{z}_{\overline{x}}^{i}))) \right) \\
            &= \frac{1}{(2m)^m} \sum_{\overline{x} \in S^m} \left( \frac{1}{T} \sum_{i=1}^{T} \operatorname{er}_{\mathbb{D}_i}(\mathcal{A}_S(\overline{z}_{\overline{x}}^{i})) \right). \\
            &\geq \min_{\overline{x} \in S^m} \frac{1}{T}  \sum_{i=1}^{T} \operatorname{er}_{\mathbb{D}_i}(\mathcal{A}_S(\overline{z}_{\overline{x}}^{i}))
        \end{align*}

        Fix an arbitrary sequence of instances $\overline{x}'=(x_1',\dots, x_m') \in S^m$. Let
        \[
            \{v_1, \dots, v_p\} \coloneq S \setminus \{x_1', \dots, x_m'\}.
        \]
        Since, $\overline{x}'$ might contain some duplicates, $|S| = 2m \leq p \leq m$. Then for any $f \in \mathcal{F}$ and $i \in \{1, \dots, T\}$, the true error of $f$ with respect to $\mathbb{D}_i$ is given by the probability of $f(x) \neq f_i(x)$ for $x \in S$:
        \[
            \operatorname{er}_{\mathbb{D}_i}(f) = \mathbb{D}_i(\mathcal{Z}_S\setminus\Gamma(f)) = \frac{1}{2m}\sum_{x \in S}\mathds{1}_{\mathcal{Z}_S\setminus\Gamma(f_i)}(x, f(x)).
        \]
        Since, $\mathds{1}_{\mathcal{Z}_S\setminus\Gamma(f_i)}(x, f(x)) \geq 0$ for all $x \in S$ and $\{v_1, \dots, v_p\} \subseteq S$, we get
        \[
            \operatorname{er}_{\mathbb{D}_i}(f) \geq \frac{1}{2m}\sum_{r=1}^p \mathds{1}_{\mathcal{Z}_S\setminus\Gamma(f_i)}(v_r, f(v_r)).
        \]
        By using, $\frac{1}{m} \geq \frac{1}{p}$,
        \[
            \operatorname{er}_{\mathbb{D}_i}(f) \geq \frac{1}{2p}\sum_{r=1}^p \mathds{1}_{\mathcal{Z}_S\setminus\Gamma(f_i)}(v_r, f(v_r)).
        \]
        For the rest of the proof, we denote $h' \coloneq \mathcal{A}_S(\overline{z}_{\overline{x}'}^{i})$ for our fixed $\overline{x}'$ for notational clarity.
        \[
            \begin{aligned}
                &\frac{1}{T}\sum_{i=1}^T\operatorname{er}_{\mathbb{D}_i}(h') \\
                &\geq \frac{1}{T}\sum_{i=1}^T \Bigl(\frac{1}{2p}\sum_{r=1}^p \mathds{1}_{\mathcal{Z}_S\setminus\Gamma(f_i)}(v_r, h'(v_r))\Bigr) \\
                &= \frac{1}{2}\Biggl(\frac{1}{p}\sum_{r=1}^p \Bigl(\frac{1}{T}\sum_{i=1}^T \mathds{1}_{\mathcal{Z}_S\setminus\Gamma(f_i)}(v_r, h'(v_r))\Bigr)\Biggr) \\
                &\geq \frac{1}{2} \min_{1 \leq r \leq p}\Bigl(\frac{1}{T}\sum_{i=1}^T \mathds{1}_{\mathcal{Z}_S\setminus\Gamma(f_i)}(v_r, h'(v_r))\Bigr),
            \end{aligned}
        \]
        where the last inequality follows from the basic fact that minimum of finite set of numbers is bounded above by the average of those numbers.
        Now, we fix an arbitrary $r \in \{1, \dots, p\}$ and evaluate the term inside the minimum:
        \[
            \frac{1}{T}\sum_{i=1}^T \mathds{1}_{\mathcal{Z}_S\setminus\Gamma(f_i)}(v_r, h'(v_r)).
        \]
        The key insight is that the hypothesis $h'$ depends on the sample $\overline{z}_{\overline{x}'}^{i}$, which in turn depends on the labels that $f_i$ assigns to the points in $\overline{x}'$. The point $v_r$ is not in the sequence $\overline{x}'$.

        We can partition the set of all functions $\mathcal{F}=\{f_1, \dots, f_T\}$ into $T/2$ disjoint pairs. Let $(f_j, f_k)$ be such a pair, constructed so that they differ only at the point $v_r$:
        \[
            f_j(x) \neq f_k(x) \iff x = v_r.
        \]
        Since $f_j$ and $f_k$ agree on all points in $\overline{x}'$, the labeled samples they generate are identical, i.e., $\overline{z}_{\overline{x}'}^{j} = \overline{z}_{\overline{x}'}^{k}$. As the learning algorithm $\mathcal{A}$ is a deterministic function, it must produce the same hypothesis for both samples. Thus, the hypothesis we denoted $h'$ is the same for both $f_j$ and $f_k$.

        This common hypothesis $h'$ makes a prediction $h'(v_r)$. Since $f_j(v_r) \neq f_k(v_r)$, $h'(v_r)$ must be different from exactly one of them. This means that for each pair, exactly one of the hypotheses is wrong at $v_r$:
        \[
            \mathds{1}_{\mathcal{Z}_S\setminus\Gamma(f_j)}(v_r, h'(v_r)) + \mathds{1}_{\mathcal{Z}_S\setminus\Gamma(f_k)}(v_r, h'(v_r)) = 1.
        \]
        Summing over all $T/2$ such pairs, the total sum of the indicators over all $T$ functions is exactly $T/2$. The average is therefore:
        \[
            \frac{1}{T}\sum_{i=1}^T \mathds{1}_{\mathcal{Z}_S\setminus\Gamma(f_i)}(v_r, h'(v_r)) = \frac{T/2}{T} = \frac{1}{2}.
        \]
        Since this holds for any $r$, the minimum over $r$ is also $1/2$. Substituting this back into our main inequality, we get:
        \[
            \frac{1}{T}\sum_{i=1}^T\operatorname{er}_{\mathbb{D}_i}(h') \geq \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4}.
        \]
        This lower bound holds for any sequence $\overline{x}' \in S^m$, so the minimum over all such sequences is also at least $1/4$. This completes the proof of the lemma.
    \end{subproof}

    With Lemma~\ref{lem:no-free-lunch} established, we know that there exists an index $j \in \{1, \dots, T\}$ such that
    \[
        \mathbb{E}_{\overline{z} \sim \mathbb{D}_j^m} \left[ \operatorname{er}_{\mathbb{D}_j}(\mathcal{A}_S(\overline{z})) \right] \ge \frac{1}{4}.
    \]
    Let us fix this index $j$ and its corresponding distribution $\mathbb{D}_j$ for the remainder of the proof. Our goal is to show that this specific distribution, when properly extended to the full sample space $\mathcal{Z}$, provides the necessary counterexample to the PAC learnability of the hypothesis space.

    We extend the distribution $\mathbb{D}_j$, which is defined on $\mathcal{Z}_S$, to a distribution $\mathbb{D}$ on the full measurable space $(\mathcal{Z}, \Sigma_{\mathcal{Z}})$ by setting
    \[
        \mathbb{D}(C) \coloneqq \mathbb{D}_j(C \cap \mathcal{Z}_S) \quad \text{for any } C \in \Sigma_{\mathcal{Z}}.
    \]
    Since $\mathbb{D}_j$ is a discrete uniform distribution on the finite set $\Gamma(f_j)$, the new distribution $\mathbb{D}$ is also a discrete uniform distribution (with the same support). Therefore, by our initial assumption, $\mathbb{D} \in \mathcal{D}$.

    This extension preserves the error calculation. For any hypothesis $h \in \mathcal{H}$, its error under $\mathbb{D}$ is the same as the error of its restriction $h|_S$ under $\mathbb{D}_j$:
    \begin{align*}
        \operatorname{er}_{\mathbb{D}}(h) &= \mathbb{D}(\mathcal{Z} \setminus \Gamma(h)) \\
        &= \mathbb{D}_j((\mathcal{Z} \setminus \Gamma(h)) \cap \mathcal{Z}_S) \\
        &= \mathbb{D}_j(\mathcal{Z}_S \setminus (\Gamma(h) \cap \mathcal{Z}_S)) \\
        &= \mathbb{D}_j(\mathcal{Z}_S \setminus \Gamma(h|_S)) = \operatorname{er}_{\mathbb{D}_j}(h|_S).
    \end{align*}
    In particular, this means $\operatorname{opt}_{\mathbb{D}}(\mathcal{H}) = 0$, since $\operatorname{er}_{\mathbb{D}}(h_j) = \operatorname{er}_{\mathbb{D}_j}(h_j|_S) = \operatorname{er}_{\mathbb{D}_j}(f_j) = 0$.

    The final step is to show that the high expected error from the lemma carries over to the full hypothesis $\mathcal{A}(\overline{z})$ under the new distribution $\mathbb{D}$.

    \begin{lemma}
        \label{lem:full-error-bound}
        Let $\mathbb{D}$ be the extended distribution defined above. Then
        \[
            \mathbb{E}_{\overline{z} \sim \mathbb{D}^m} \left[ \operatorname{er}_{\mathbb{D}}(\mathcal{A}(\overline{z})) \right] \ge \frac{1}{4}.
        \]
    \end{lemma}
\end{proof}